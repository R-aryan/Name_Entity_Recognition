{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NER_DiseaeName.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPrIrAENv7YaFi6hoxUyHTT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/R-aryan/Name_Entity_Recognition/blob/master/NER_DiseaeName_Transfer_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-aBaXpZYd7XP",
        "colab_type": "code",
        "outputId": "4856fd26-710f-4ba7-8c5f-cf4f065de246",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(\"Hello colab\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hello colab\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SPpWpwD3BmJ",
        "colab_type": "code",
        "outputId": "50193228-f7e2-4c1f-a333-8efb0bcb1161",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfaICsYE3Cd7",
        "colab_type": "code",
        "outputId": "0ab2929b-b795-47b0-835a-24fa674dd18a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 627
        }
      },
      "source": [
        "!pip install -r '/content/drive/My Drive/Colab Notebooks/requirements.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: absl-py==0.9.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 1)) (0.9.0)\n",
            "Collecting astor==0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/35/6b/11530768cac581a12952a2aad00e1526b89d242d0b9f59534ef6e6a1752f/astor-0.7.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: attrs==19.3.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 3)) (19.3.0)\n",
            "Collecting awscli==1.17.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4e/dc/d44fabd4a84eb18353ce51defc87f84ace88a24fda085cb5699251dda627/awscli-1.17.7-py2.py3-none-any.whl (2.8MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8MB 7.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: backcall==0.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 5)) (0.1.0)\n",
            "Requirement already satisfied: bleach==3.1.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 6)) (3.1.0)\n",
            "Collecting blis==0.4.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/41/19/f95c75562d18eb27219df3a3590b911e78d131b68466ad79fdf5847eaac4/blis-0.4.1-cp36-cp36m-manylinux1_x86_64.whl (3.7MB)\n",
            "\u001b[K     |████████████████████████████████| 3.7MB 44.5MB/s \n",
            "\u001b[?25hCollecting boto3==1.11.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/4f/a75bf88d30b79e08dd7930d9643aded0a79c9f696b307f353bb33f62cc5b/boto3-1.11.7-py2.py3-none-any.whl (128kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 69.8MB/s \n",
            "\u001b[?25hCollecting botocore==1.14.7\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/10/d7/da416c5d6a8c61796dbc37875c886955798ca88385882725049af49d417a/botocore-1.14.7-py2.py3-none-any.whl (5.9MB)\n",
            "\u001b[K     |████████████████████████████████| 5.9MB 41.3MB/s \n",
            "\u001b[?25hCollecting catalogue==1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/6c/f9/9a5658e2f56932e41eb264941f9a2cb7f3ce41a80cb36b2af6ab78e2f8af/catalogue-1.0.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: certifi==2019.11.28 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 11)) (2019.11.28)\n",
            "Requirement already satisfied: cffi==1.13.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 12)) (1.13.2)\n",
            "Requirement already satisfied: chardet==3.0.4 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 13)) (3.0.4)\n",
            "Collecting colorama==0.4.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/dc/45cdef1b4d119eb96316b3117e6d5708a08029992b2fee2c143c7a0a5cc5/colorama-0.4.3-py2.py3-none-any.whl\n",
            "Collecting conllu==2.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/6b/94/9d5b8f473d24d3df1d0b8410095871a2fd4c8d5b817a071025ba63ff2b38/conllu-2.2.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 16)) (0.10.0)\n",
            "Requirement already satisfied: cymem==2.0.3 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 17)) (2.0.3)\n",
            "Requirement already satisfied: decorator==4.4.1 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 18)) (4.4.1)\n",
            "Requirement already satisfied: defusedxml==0.6.0 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 19)) (0.6.0)\n",
            "Requirement already satisfied: docutils==0.15.2 in /usr/local/lib/python3.6/dist-packages (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 20)) (0.15.2)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement en-ner-bc5cdr-md==0.2.4 (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 21)) (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for en-ner-bc5cdr-md==0.2.4 (from -r /content/drive/My Drive/Colab Notebooks/requirements.txt (line 21))\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mmiYeTy5MnNw",
        "colab_type": "code",
        "outputId": "ec80fb0c-4e51-4587-ebb4-a13167261762",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "source": [
        "pip install pytorch-pretrained-bert==0.4.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (2.21.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.3.1)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.10.47)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (4.28.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from pytorch-pretrained-bert==0.4.0) (1.17.5)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->pytorch-pretrained-bert==0.4.0) (2.8)\n",
            "Requirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (0.2.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (0.9.4)\n",
            "Requirement already satisfied: botocore<1.14.0,>=1.13.47 in /usr/local/lib/python3.6/dist-packages (from boto3->pytorch-pretrained-bert==0.4.0) (1.13.50)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (2.6.1)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (0.15.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\"->botocore<1.14.0,>=1.13.47->boto3->pytorch-pretrained-bert==0.4.0) (1.12.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoobRHKvTBo9",
        "colab_type": "code",
        "outputId": "28445e96-7999-4e3f-9e55-1a0b56f5f5fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 748
        }
      },
      "source": [
        "pip install scispacy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scispacy in /usr/local/lib/python3.6/dist-packages (0.2.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.14.1)\n",
            "Requirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.2.3)\n",
            "Requirement already satisfied: pysbd in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.2.3)\n",
            "Requirement already satisfied: conllu in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.2.1)\n",
            "Requirement already satisfied: awscli in /usr/local/lib/python3.6/dist-packages (from scispacy) (1.17.9)\n",
            "Requirement already satisfied: scikit-learn>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from scispacy) (0.22.1)\n",
            "Requirement already satisfied: nmslib>=1.7.3.6 in /usr/local/lib/python3.6/dist-packages (from scispacy) (2.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from scispacy) (1.17.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (1.0.2)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (0.4.1)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (7.3.1)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (3.0.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (0.9.6)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (0.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (42.0.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (2.0.3)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (1.0.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->scispacy) (1.0.1)\n",
            "Requirement already satisfied: colorama<0.4.2,>=0.2.5 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (0.4.1)\n",
            "Requirement already satisfied: PyYAML<5.3,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (3.13)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (0.15.2)\n",
            "Requirement already satisfied: botocore==1.14.9 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (1.14.9)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (0.3.2)\n",
            "Requirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli->scispacy) (3.4.2)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.3->scispacy) (1.4.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (5.4.8)\n",
            "Requirement already satisfied: pybind11>=2.2.3 in /usr/local/lib/python3.6/dist-packages (from nmslib>=1.7.3.6->scispacy) (2.4.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.1->scispacy) (4.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (1.4.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (2019.11.28)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->scispacy) (3.0.4)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.14.9->awscli->scispacy) (2.6.1)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.14.9->awscli->scispacy) (0.9.4)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli->scispacy) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (1.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore==1.14.9->awscli->scispacy) (1.12.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->scispacy) (8.0.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rt1MHCk1QDI7",
        "colab_type": "code",
        "outputId": "43ce49bc-d95f-4bfe-a845-ac7b1c41ba16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 540
        }
      },
      "source": [
        "pip install '/content/drive/My Drive/Colab Notebooks/en_ner_bc5cdr_md-0.2.4.tar.gz'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Processing ./drive/My Drive/Colab Notebooks/en_ner_bc5cdr_md-0.2.4.tar.gz\n",
            "Requirement already satisfied: spacy>=2.2.1 in /usr/local/lib/python3.6/dist-packages (from en-ner-bc5cdr-md==0.2.4) (2.2.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.21.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=0.1.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.1)\n",
            "Requirement already satisfied: thinc<7.4.0,>=7.3.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (7.3.1)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (0.6.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.17.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (0.9.6)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (3.0.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (42.0.2)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.24.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.4.0,>=7.3.0->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (4.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.4.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (1.0.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from zipp>=0.5->importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.1->en-ner-bc5cdr-md==0.2.4) (8.0.2)\n",
            "Building wheels for collected packages: en-ner-bc5cdr-md\n",
            "  Building wheel for en-ner-bc5cdr-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-ner-bc5cdr-md: filename=en_ner_bc5cdr_md-0.2.4-cp36-none-any.whl size=70531466 sha256=857ac063413e467852cf324a9b984a9d3d18802767279a0208c74e8a91b9962b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/07/ca/bda9e50b07b11aa9a6e51441a9dda435560fc548957a31b81e\n",
            "Successfully built en-ner-bc5cdr-md\n",
            "Installing collected packages: en-ner-bc5cdr-md\n",
            "Successfully installed en-ner-bc5cdr-md-0.2.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uLCQmchoQvRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jwwSIcyF-qKe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"ggplot\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow_hub as hub\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "from keras.models import Model, Input\n",
        "from keras.layers import LSTM, Embedding, Dense\n",
        "from keras.layers import TimeDistributed, Dropout, Bidirectional\n",
        "\n",
        "from tqdm import tqdm, trange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZSVCQEF_KMR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Defining Constants\n",
        " \n",
        "# Maximum length of text sentences\n",
        "MAXLEN = 180\n",
        "\n",
        "# batch size\n",
        "BS=48"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7w3jWOW_T7i",
        "colab_type": "code",
        "outputId": "9e139580-5067-4bfd-9dcc-f3307aeedba8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355
        }
      },
      "source": [
        "data_test = pd.read_csv(\"/content/drive/My Drive/NER_Disease/train.csv\", encoding=\"latin1\")\n",
        "data1 = data_test.sample(n=15000,random_state=2020)\n",
        "data1.tail(10)\n",
        "\n",
        "# data1=data.sample(n=25000,random_state=2020)\n",
        "# data1.head(10)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>Doc_ID</th>\n",
              "      <th>Sent_ID</th>\n",
              "      <th>Word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2059180</th>\n",
              "      <td>2059181</td>\n",
              "      <td>13684</td>\n",
              "      <td>87015</td>\n",
              "      <td>adhesion</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4386570</th>\n",
              "      <td>4386571</td>\n",
              "      <td>28960</td>\n",
              "      <td>184768</td>\n",
              "      <td>&lt;</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3864848</th>\n",
              "      <td>3864849</td>\n",
              "      <td>25525</td>\n",
              "      <td>162691</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4106376</th>\n",
              "      <td>4106377</td>\n",
              "      <td>27126</td>\n",
              "      <td>172912</td>\n",
              "      <td>5-HT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1403106</th>\n",
              "      <td>1403107</td>\n",
              "      <td>9314</td>\n",
              "      <td>59117</td>\n",
              "      <td>in</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3287894</th>\n",
              "      <td>3287895</td>\n",
              "      <td>21719</td>\n",
              "      <td>138403</td>\n",
              "      <td>in</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4195244</th>\n",
              "      <td>4195245</td>\n",
              "      <td>27724</td>\n",
              "      <td>176684</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2475003</th>\n",
              "      <td>2475004</td>\n",
              "      <td>16324</td>\n",
              "      <td>104084</td>\n",
              "      <td>follow</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1521318</th>\n",
              "      <td>1521319</td>\n",
              "      <td>10130</td>\n",
              "      <td>64195</td>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2499807</th>\n",
              "      <td>2499808</td>\n",
              "      <td>16493</td>\n",
              "      <td>105121</td>\n",
              "      <td>care</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              id  Doc_ID  Sent_ID      Word tag\n",
              "2059180  2059181   13684    87015  adhesion   O\n",
              "4386570  4386571   28960   184768         <   O\n",
              "3864848  3864849   25525   162691       the   O\n",
              "4106376  4106377   27126   172912      5-HT   O\n",
              "1403106  1403107    9314    59117        in   O\n",
              "3287894  3287895   21719   138403        in   O\n",
              "4195244  4195245   27724   176684       the   O\n",
              "2475003  2475004   16324   104084    follow   O\n",
              "1521318  1521319   10130    64195       the   O\n",
              "2499807  2499808   16493   105121      care   O"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw4ZbP12_eiN",
        "colab_type": "code",
        "outputId": "31e81623-5e3e-4cb7-a33b-d28467918127",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "words = list(set(data1[\"Word\"].values))\n",
        "words.append(\"ENDPAD\")\n",
        "n_words = len(words); n_words\n",
        "#print(words[1:10])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5260"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1OsaOQW_kEf",
        "colab_type": "code",
        "outputId": "34e06b58-93e4-4cde-c345-fb1b48d542ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tags = list(set(data1[\"tag\"].values))\n",
        "n_tags = len(tags); n_tags\n",
        "print(tags[0:n_tags])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['O', 'I-indications', 'B-indications']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "imLkNhAHGH4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L1ld8O1D_n0f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdp7A4UYK-B0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentenceGetter(object):\n",
        "    \n",
        "    def __init__(self, data):\n",
        "        self.n_sent = 1\n",
        "        self.data = data\n",
        "        self.empty = False\n",
        "        agg_func = lambda s: [(w, t) for w, t in zip(s[\"Word\"].values.tolist(),\n",
        "                                                           s[\"tag\"].values.tolist())]\n",
        "        self.grouped = self.data.groupby(\"Sent_ID\").apply(agg_func)\n",
        "        self.sentences = [s for s in self.grouped]\n",
        "    \n",
        "    def get_next(self):\n",
        "        try:\n",
        "            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n",
        "            self.n_sent += 1\n",
        "            return s\n",
        "        except:\n",
        "            return None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jNKCE4DMD-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "getter = SentenceGetter(data1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zW1lsfagMG7B",
        "colab_type": "code",
        "outputId": "e33e455c-c0cb-4fdf-fc67-4ecbd6cf8770",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
        "sentences[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SW620'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOt9i27LMOJr",
        "colab_type": "code",
        "outputId": "296772a7-c89e-4b30-ddaa-b59c301a01ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
        "print(len(labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "14247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eK9PlhdVMRhb",
        "colab_type": "code",
        "outputId": "0b2a18b1-4ded-4beb-ff4c-641a7bdaf545",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tags_vals = list(set(data1[\"tag\"].values))\n",
        "tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "\n",
        "print(tag2idx)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'O': 0, 'I-indications': 1, 'B-indications': 2}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouR5U9iXMUHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.optim import Adam\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertConfig\n",
        "from pytorch_pretrained_bert import BertForTokenClassification, BertAdam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlWaEBr1MWid",
        "colab_type": "code",
        "outputId": "227215ea-adfc-498a-98ad-7ffdfe285ded",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "device"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1607W5iKM4O9",
        "colab_type": "text"
      },
      "source": [
        "The Bert implementation comes with a pretrained tokenizer and a definied vocabulary. We load the one related to the smallest pre-trained model bert-base-uncased.  cased variate since  is well suited for NER."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lURJxUZ7M5vR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aDRT8euKNCSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VY1Tt1sHNEuK",
        "colab_type": "text"
      },
      "source": [
        "Now we tokenize all sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zszDUpkfNHjr",
        "colab_type": "code",
        "outputId": "1e6ab6bf-d703-4d7c-eff9-d36171d2e33a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "print(tokenized_texts[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sw', '##6', '##20']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X34Us00KNJb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyZSGdnJNp0k",
        "colab_type": "text"
      },
      "source": [
        "Next, we cut and pad the token and label sequences to our desired length. That is Next, we need to convert each token in each review to an id as present in the tokenizer vocabulary. If there’s a token that is not present in the vocabulary, the tokenizer will use the special [UNK] token and use its id:\n",
        "\n",
        "Refer:-  https://towardsdatascience.com/bert-to-the-rescue-17671379687f"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zZAX5YqENq46",
        "colab_type": "code",
        "outputId": "a9803599-caef-4cd5-e706-6865ce213642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAXLEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "input_ids[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([25430,  2575, 11387,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "           0,     0,     0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sysA7hRjNxxV",
        "colab_type": "code",
        "outputId": "76098588-67cf-4119-de80-6741adef68a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAXLEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "len(tags)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14247"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jAiS6QuN2qy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2PDzodWN6K4",
        "colab_type": "text"
      },
      "source": [
        "The Bert model supports something called attention_mask, which is similar to the masking in keras. So here we create the mask to ignore the padded elements in the sequences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nS48sdf1N7K9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "\n",
        "#attention_masks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xFg7d339N_UI",
        "colab_type": "code",
        "outputId": "7d913ef4-1f1f-42e1-e2ed-b1c0b42a15a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "data1[\"tag\"].values"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['O', 'O', 'O', ..., 'O', 'O', 'O'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQO3_ibhOFvD",
        "colab_type": "text"
      },
      "source": [
        "Now we split the dataset to use 10% to validate the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RI1YdT2rOHAe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2018,test_size=0.05)\n",
        "tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018,test_size=0.05)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0DiUdO2OM4G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OoiB1pL0ORLQ",
        "colab_type": "text"
      },
      "source": [
        "Since we’re operating in pytorch, we have to convert the dataset to torch tensors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jW0S14vHOSHB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tr_inputs = torch.tensor(tr_inputs).to(torch.int64)\n",
        "val_inputs = torch.tensor(val_inputs).to(torch.int64)\n",
        "tr_tags = torch.tensor(tr_tags).to(torch.int64)\n",
        "val_tags = torch.tensor(val_tags).to(torch.int64)\n",
        "tr_masks = torch.tensor(tr_masks).to(torch.int64)\n",
        "val_masks = torch.tensor(val_masks).to(torch.int64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0pRDFaMOVIN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a549RNjMOYxk",
        "colab_type": "text"
      },
      "source": [
        "The last step is to define the dataloaders. We shuffle the data at training time with the RandomSampler and at test time we just pass them sequentially with the SequentialSampler."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqoT17ipOZvu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tvw1UokZOcqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCtNdIGKOhTW",
        "colab_type": "text"
      },
      "source": [
        "## Setup the Bert model for finetuning\n",
        "\n",
        "The pytorch-pretrained-bert package provides a BertForTokenClassification class for token-level predictions. BertForTokenClassification is a fine-tuning model that wraps BertModel and adds token-level classifier on top of the BertModel. The token-level classifier is a linear layer that takes as input the last hidden state of the sequence. We load the pre-trained bert-base-uncased model and provide the number of possible labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jhd4BLY-OiT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sA96nDjEOl2S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# print(\"Model's state_dict:\")\n",
        "# for param_tensor in model.state_dict():\n",
        "#     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "import os\n",
        "\n",
        "\n",
        "\n",
        "try:\n",
        "  corpus = \"/content/drive/My Drive/Colab Notebooks/bert_epoch_state.pt\"\n",
        "  checkpoint = torch.load(corpus)\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  epoch = checkpoint['epoch']\n",
        "  loss = checkpoint['loss']\n",
        "\n",
        "\n",
        "\n",
        "except:\n",
        "  pass\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAsNWZ_7jX62",
        "colab_type": "code",
        "outputId": "4c4dd082-07fc-47da-e882-9df1532ff0fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/Colab Notebooks/bert_epoch_state.pt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJ3J3aF7OsKN",
        "colab_type": "text"
      },
      "source": [
        "Now we have to pass the model parameters to the CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BeDRwzn0Os2x",
        "colab_type": "code",
        "outputId": "c05cf088-4b6e-41d5-edaa-ce98b28cd5c3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.cpu()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwm2y7OmOvuR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EwSKp6BhO0d3",
        "colab_type": "text"
      },
      "source": [
        "Before we can start the fine-tuning process, we have to setup the optimizer and add the parameters it should update. A common choice is the Adam optimizer. We also add some weight_decay as regularization to the main weight matrices. If you have limited resources, you can also try to just train the linear classifier on top of Bert and keep all other weights fixed. This will still give you a good performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BWxSosveO1Y0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "FULL_FINETUNING = True\n",
        "if FULL_FINETUNING:\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    #print(param_optimizer)\n",
        "    no_decay = ['bias', 'gamma', 'beta']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.01},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0}\n",
        "    ]\n",
        "else:\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pK99EbQaO42f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HziPhCeO_wu",
        "colab_type": "text"
      },
      "source": [
        "## Finetune Bert\n",
        "\n",
        "First we define some metrics, we want to track while training. We use the f1_score from the seqeval package.And we use simple accuracy on a token level comparable to the accuracy in keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vrqri8jxPArO",
        "colab_type": "code",
        "outputId": "8f4dd67d-49e5-44c7-fce6-04675ed7abca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "source": [
        "pip install seqeval"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from seqeval) (1.17.5)\n",
            "Requirement already satisfied: Keras>=2.2.4 in /usr/local/lib/python3.6/dist-packages (from seqeval) (2.2.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.1.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras>=2.2.4->seqeval) (1.4.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=75dcc99864bdc5231b6e846311cc3ea63cdd6e81ceb1a654530ba7c6af6fbf4e\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-0.0.12\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgq-zCXRPDU0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from seqeval.metrics import f1_score\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=2).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j4ll9ZLaPSvf",
        "colab_type": "code",
        "outputId": "4861868a-1a0b-41c5-e083-22ae7b8ec098",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.cuda(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2B7uTjaPWze",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can fine-tune the model. A few epochs should be enough. The paper suggest 3-4 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-iKQNmbPXvL",
        "colab_type": "code",
        "outputId": "a7a93c8e-c5d6-47f5-b346-47a9bcfc4d0b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 485
        }
      },
      "source": [
        "epochs = 5\n",
        "max_grad_norm = 1.0\n",
        "\n",
        "model.cuda(device)\n",
        "\n",
        "for epoch in trange(epochs, desc=\"Epoch\"):\n",
        "\n",
        "    # TRAIN loop\n",
        "    model.train()\n",
        "\n",
        "    tr_loss = 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # add batch to gpu\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "        # forward pass\n",
        "        loss = model(b_input_ids, token_type_ids=None,\n",
        "                     attention_mask=b_input_mask, labels=b_labels)\n",
        "        \n",
        "        # backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # track train loss\n",
        "        tr_loss += loss.item()\n",
        "        nb_tr_examples += b_input_ids.size(0)\n",
        "        nb_tr_steps += 1\n",
        "\n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "        # update parameters\n",
        "        optimizer.step()\n",
        "        model.zero_grad()\n",
        "\n",
        "    # print train loss per epoch\n",
        "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "\n",
        "\n",
        "\n",
        "    # VALIDATION on validation set\n",
        "    model.eval()\n",
        "\n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    predictions , true_labels = [], []\n",
        "\n",
        "    for batch in valid_dataloader:\n",
        "\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        with torch.no_grad():\n",
        "\n",
        "            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                                  attention_mask=b_input_mask, labels=b_labels)\n",
        "            \n",
        "            logits = model(b_input_ids, token_type_ids=None,\n",
        "                           attention_mask=b_input_mask)\n",
        "            \n",
        "        logits = logits.detach().cpu().numpy()\n",
        "\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "\n",
        "        true_labels.append(label_ids)\n",
        "        \n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        eval_loss += tmp_eval_loss.mean().item()\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "        \n",
        "        nb_eval_examples += b_input_ids.size(0)\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    eval_loss = eval_loss/nb_eval_steps\n",
        "\n",
        "    print(\"Validation loss: {}\".format(eval_loss))\n",
        "\n",
        "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "\n",
        "    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n",
        "    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n",
        "\n",
        "    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "\n",
        "\n",
        "   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "torch.save(model.state_dict(), '/content/drive/My Drive/NER_Disease/my_NER_model.h5')\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.010535766553129874\n",
            "Validation loss: 0.003995094417326111\n",
            "Validation Accuracy: 0.9995652173913046\n",
            "F1-Score: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  20%|██        | 1/5 [03:41<14:44, 221.19s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0026672742769948314\n",
            "Validation loss: 0.003629560764871396\n",
            "Validation Accuracy: 0.9995652173913046\n",
            "F1-Score: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  40%|████      | 2/5 [07:22<11:03, 221.32s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.002460509297838677\n",
            "Validation loss: 0.0034427625064183858\n",
            "Validation Accuracy: 0.9995652173913046\n",
            "F1-Score: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  60%|██████    | 3/5 [11:04<07:22, 221.36s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0018797821331617585\n",
            "Validation loss: 0.0018063113915875715\n",
            "Validation Accuracy: 0.9995652173913046\n",
            "F1-Score: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch:  80%|████████  | 4/5 [14:45<03:41, 221.38s/it]\u001b[A"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train loss: 0.0010435409283277306\n",
            "Validation loss: 0.0017941044031787264\n",
            "Validation Accuracy: 0.9995652173913044\n",
            "F1-Score: 0.5238095238095238\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Epoch: 100%|██████████| 5/5 [18:27<00:00, 221.37s/it]\u001b[A\n",
            "\u001b[A"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "coDJBz38lX9O",
        "colab_type": "code",
        "outputId": "abcad970-a35c-4395-981a-8bc07d36cd22",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/bert_state_dict_partial.pt'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymFcpDGpGLcg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "checkpoint = torch.load('/content/drive/My Drive/Colab Notebooks/bert_epoch_state_full.pt')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "epoch = checkpoint['epoch']\n",
        "loss = checkpoint['loss']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-0AgKBgRUJO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def preprocess_test_data(data):\n",
        "    words = list(set(data[\"Word\"].values))\n",
        "    words.append(\"ENDPAD\")\n",
        "    n_words = len(words); n_words\n",
        "    #print(words[1:10])\n",
        "    \n",
        "    \n",
        "    tags = list(set(data[\"tag\"].values))\n",
        "    n_tags = len(tags); n_tags\n",
        "    print(tags[0:n_tags])\n",
        "    \n",
        "    \n",
        "    getter = SentenceGetter(data)\n",
        "    \n",
        "    \n",
        "    sentences = [\" \".join([s[0] for s in sent]) for sent in getter.sentences]\n",
        "    sentences[23]\n",
        "    \n",
        "    \n",
        "    labels = [[s[1] for s in sent] for sent in getter.sentences]\n",
        "    print(len(labels))\n",
        "    \n",
        "    \n",
        "    \n",
        "    tags_vals = list(set(data[\"tag\"].values))\n",
        "    tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
        "\n",
        "    print(tag2idx)\n",
        "\n",
        "\n",
        "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "    print(tokenized_texts)\n",
        "\n",
        "\n",
        "\n",
        "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
        "                          maxlen=MAXLEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "    input_ids[0]\n",
        "\n",
        "\n",
        "    tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n",
        "                     maxlen=MAXLEN, value=tag2idx[\"O\"], padding=\"post\",\n",
        "                     dtype=\"long\", truncating=\"post\")\n",
        "\n",
        "    len(tags)\n",
        "\n",
        "  \n",
        "    attention_masks = [[float(i>0) for i in ii] for ii in input_ids]\n",
        "\n",
        "    #attention_masks\n",
        "\n",
        "\n",
        "\n",
        "    tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n",
        "                                                            random_state=2019, test_size=0.9)\n",
        "    tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2019, test_size=0.9)\n",
        "\n",
        "\n",
        "    tr_inputs = torch.tensor(tr_inputs).to(torch.int64)\n",
        "    val_inputs = torch.tensor(val_inputs).to(torch.int64)\n",
        "    tr_tags = torch.tensor(tr_tags).to(torch.int64)\n",
        "    val_tags = torch.tensor(val_tags).to(torch.int64)\n",
        "    tr_masks = torch.tensor(tr_masks).to(torch.int64)\n",
        "    val_masks = torch.tensor(val_masks).to(torch.int64)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    test_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "    test_sampler = SequentialSampler(test_data)\n",
        "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=bs)\n",
        "\n",
        "\n",
        "    return test_dataloader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FC65H3Kt1o3f",
        "colab_type": "code",
        "outputId": "4d7855e8-c37e-47ee-ff05-d4c9d65c17c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "data_test = pd.read_csv(\"/content/drive/My Drive/NER_Disease/train.csv\", encoding=\"latin1\")\n",
        "data_test = data_test.fillna(method=\"ffill\")\n",
        "data.tail(10)\n",
        "\n",
        "data_sample=data_test.sample(n=1000,random_state=1234)\n",
        "data_sample.tail(10)\n",
        "\n",
        "test_dataloader= preprocess_test_data(data_sample)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['B-indications', 'O', 'I-indications']\n",
            "995\n",
            "{'B-indications': 0, 'O': 1, 'I-indications': 2}\n",
            "[['significantly'], ['0', '.', '64'], ['and'], ['def', '##ae', '##cation'], ['abnormal', '##ity'], [','], ['.'], ['hybrid', '-', 'cell'], ['at'], ['of'], ['differential'], ['organ', '##ome', '##tal', '##lic'], ['a'], ['field'], ['paint', '##brush'], ['neuroscience'], ['these'], ['fl', '##a', '.'], ['complex'], ['species'], ['be'], ['had'], ['('], ['ve', '##rte', '##brates'], ['lines'], ['p'], ['also'], ['('], [','], ['of'], ['('], ['homosexuality'], ['nucleus'], ['cellular'], ['overall'], ['do'], ['in'], ['.'], ['were'], [','], ['have'], ['to'], ['treatment'], ['and'], ['pl', '##oid', '##y'], ['with'], ['also'], ['acid'], ['group'], ['fertile'], ['may'], ['without'], ['2018'], ['find'], ['der'], ['yet'], ['both'], ['.'], ['at'], ['and'], ['self', '-', 'administration'], ['poly', '##p'], ['up'], ['er', '##ite', '##mat', '##oso'], ['do', '##pa', '##mine', '##rg', '##ic'], ['multiple'], ['item', '-', 'scale'], ['amounts'], ['uses'], ['and'], ['tests'], ['using'], ['in'], ['amp', '##c'], ['in'], ['im', '##mis', '##ci', '##ble'], [')'], ['looking'], ['behavioral'], ['('], ['who'], ['dissemination'], ['tuberculosis'], ['of'], ['transformed'], ['of'], [','], ['ultra', '##sonic', '/', 'p', '##ne', '##umatic'], ['baby', '-', 'feeding'], ['interviewed'], ['on'], ['is'], [\"'\", 's'], ['as'], ['do', '##bu', '##tam', '##ine'], ['album', '##in', '##uria'], ['wind'], ['reconstruction'], ['rear', '##rang', '##ement', '##s'], ['this'], [','], ['ap', '##o'], [')'], ['elevated'], ['cyclic'], ['of'], ['the'], ['reported'], ['('], ['location'], ['0', ',', '85'], ['ge', '##rmin', '##al'], ['disease'], ['pregnant'], ['of'], ['as'], ['hyper', '##al', '##ges', '##ia'], ['research'], ['of'], ['study'], ['media', '##ting'], ['cells'], ['35', '/', '69'], ['motif'], ['2'], ['carried'], ['to'], ['experimental', '##ly'], [','], ['proton'], ['of'], ['cancer'], ['cellular'], [','], ['.'], ['ac', '##l', '-', 'intact'], ['phases'], ['.'], [','], ['en', '##cam', '##in', '##ham', '##ent', '##o'], ['however'], ['between'], ['treatment'], ['technique'], ['b'], ['career'], ['used'], ['analysis'], ['que'], ['different'], ['that'], ['0', '.', '03'], [','], ['intervention', '##al', '-', 'based'], ['2001'], ['ec', '##top', '##ic'], ['might'], ['.'], ['the'], ['por', '##cine'], ['review'], ['('], ['mc', '##od'], ['blast', '##ula'], ['role'], [']'], ['material'], ['grade'], ['these'], ['a'], ['and'], ['induced'], ['an'], ['undergoing'], ['he', '##pa', '##to', '##cellular'], ['of'], ['contributing'], ['the'], ['for'], ['at'], ['significantly'], ['the'], ['rec', '##urrent'], ['and'], [','], ['models'], ['that'], ['of'], ['check', '##off'], [','], ['.'], ['group'], ['supporting'], ['national'], ['order'], ['european'], ['with'], ['the'], [')'], [','], ['available'], ['however'], ['effectiveness'], ['in'], [','], ['from'], ['('], ['7', '.', '1'], ['syn', '##ap', '##tic'], ['%'], ['ga', '##ting'], [')'], ['au', '##reus'], ['9', '.', '1'], ['gel', '##ech', '##iidae'], ['stopping'], ['de'], ['which'], ['mg'], ['syn', '##m'], ['to'], ['dc'], ['telephone'], ['matched'], ['surface'], ['.'], ['practical'], ['at'], ['survival'], [')'], ['.'], ['('], ['detector'], ['phases'], ['co', '##fa', '##ctor'], ['their'], ['`', '`'], ['formation'], ['in'], ['on'], ['of'], ['used'], ['was'], ['-', 'ad', '##t', '##n'], ['we'], ['numb'], ['a'], ['.'], ['the'], ['message'], ['cells'], ['new'], ['tr', '##af', '##imo', '##w'], ['of'], ['and'], ['a'], ['trial'], ['carried'], ['a'], ['compliance'], ['inhibitors'], [')'], ['long', '-', 'term'], ['we'], ['and'], ['.'], ['treatment'], ['.'], ['with'], ['drugs'], ['the'], ['.'], ['('], ['nigeria'], ['improvement'], ['these'], ['dental'], ['corn', '##ea'], ['this'], ['the'], ['ass', '##ay'], ['lines'], ['show'], ['dental'], ['were'], ['rec', '##a'], ['('], ['pi', '##3', '##k'], [','], ['efficient'], ['.'], ['in'], ['experiments'], ['volumes'], ['.'], ['bio', '##ps', '##ies'], ['and'], ['the'], ['conclude'], ['from'], ['the'], ['10'], ['to'], ['.'], ['dependence'], ['was'], ['of'], ['investigating'], ['p', '##yr', '##id', '##yla', '##minated'], ['on'], ['type'], ['more'], ['camp'], ['am', '##bula', '##tory'], ['and'], ['the'], [','], ['h', '##la', '-', 'dr'], ['in', '##fect'], ['typing'], ['on'], ['b'], ['activity'], ['discussed'], ['a'], ['of'], ['immediate'], ['risk'], ['intra', '##per', '##ito', '##nea', '##l'], ['emphasis'], ['with'], ['are'], ['in'], ['of'], ['.'], ['.'], ['12'], ['permanent'], ['au', '##c'], ['.'], ['my', '##oca', '##rdial'], ['theories'], ['receptors'], ['rye', '##grass'], ['designed'], [','], ['a'], ['first', '-', 'order'], ['of'], ['in'], ['were'], ['a'], ['a'], ['completely'], ['indicated'], ['this'], ['and'], ['the'], ['from'], ['pit'], ['('], ['%'], ['.'], ['situ', '##aa', '§', 'a', '##µ', '##es'], ['p'], ['subunit', '##s'], ['similar'], ['and'], ['these'], ['allows'], ['ur', '##ina', '##ry'], [','], ['strongly'], ['95'], ['pa', '##pi', '##llo', '##ma', '##virus'], ['2003'], ['hearing'], ['the'], ['cord'], ['mono', '-'], ['a'], ['and'], ['role'], [','], ['all'], ['into'], ['similar'], ['['], ['of'], ['('], ['neither'], ['the'], [')'], ['ul', '##cer'], ['thy', '##ro', '##xin'], ['61'], ['tu', '##mour'], ['registers'], ['is'], ['infections'], ['taken'], ['profile'], ['gr', '##iff'], ['l', '##hr', '##h', '-', 'r'], [';'], ['be'], ['foot'], ['was'], ['properties'], ['.'], ['cc', '##aa', '##t'], ['anti', '##sul', '##fat', '##ide'], ['qualities'], ['os', '##si', '##fication'], ['with'], ['u', '##u', '##u'], ['chinese'], ['development'], ['a'], ['completely'], ['ce', '##ti', '##l'], ['was'], ['and'], [','], ['presented'], ['resistance'], ['supports'], ['are'], ['rice'], ['light'], ['biology'], ['d', '##ys', '##pha', '##gia'], ['20'], ['ser', '##one', '##gative'], ['reversed'], ['a'], ['after'], ['although'], ['of'], ['a'], ['cells'], ['the'], ['purposes'], ['a'], ['building'], ['older'], ['the'], ['scale'], ['do', '##rso', '##lateral'], ['age'], ['elevated'], ['or'], ['.'], ['ph', '##arm', '##aco', '##logical'], ['the'], ['when'], ['any'], ['importance'], ['organization'], ['patients'], [')'], ['inserted'], ['1'], ['the'], ['2', '+'], [','], ['maize'], ['measured'], [','], ['%'], ['tal', '##ar'], ['protein'], ['relief'], ['gi', '##sts'], [')'], ['level'], ['isolated'], ['base', '##plate'], ['with'], ['is'], ['the'], ['pre', '##nat', '##al'], ['list', \"'\", \"'\"], ['practice'], ['0', '.', '57', '##3'], ['%'], ['in'], ['absorption'], ['reviewed'], ['evaluation'], ['was'], ['outcome'], ['.'], ['men'], ['in'], ['semi', '-', 'quantitative'], ['interaction'], ['.'], ['using'], ['sea', '##col', '##e'], ['has'], ['mc', '##i'], ['in'], [')'], ['fluid'], ['citrus'], ['200'], [','], ['auto', '##im', '##mun', '##ity'], ['state', 'and'], ['since'], [')'], [':'], ['es', '##rd'], ['inhibition'], ['in'], ['levels'], ['and'], [','], ['universally'], ['explore'], ['surrounding'], ['original'], ['on'], ['release'], ['the'], ['word'], ['survival'], [';'], ['sp', '##ut', '##um'], ['odds'], ['fibre'], ['suggests'], ['trans', '##du', '##cer'], ['6'], ['.'], ['into'], ['investigated'], ['microscopy'], ['up'], ['by'], ['aa', '##©', '##rea'], ['aspects'], ['reference'], ['is'], ['current'], ['of'], ['and'], [','], ['gen', '##omic'], ['24'], ['administered'], ['b'], ['through'], ['normal'], ['with'], ['frequency'], ['-', 'cat', '##aly', '##zed'], ['predict', '##or'], ['not'], ['2', '-', 'fold'], ['processes'], ['directly'], ['disability'], ['based'], ['oxide'], ['of'], ['viruses'], ['the'], [','], ['.'], ['micro', '##tub', '##ule'], ['effect'], ['on'], ['positive'], [')'], ['.'], ['of'], ['distributed'], ['factors'], ['discussions'], ['learning', '-', 'related'], ['sperm', '##ato', '##genic'], ['patients'], ['rec', '##ur', '##rence'], ['and'], ['he', '##xa', '##ch', '##lor', '##obe', '##nz', '##ene'], ['153'], ['cap', '##illa', '##ry'], ['to'], ['glucose'], ['reviewed'], ['pd', '##gf', '-', 'b'], ['prevent'], ['una'], ['increased'], ['pre', '##tre', '##ated'], ['effect'], ['dc', '##g'], ['subjected'], ['students'], ['.'], ['of'], ['reflex', '##a'], ['exclusion'], ['at'], ['this'], ['drinking'], ['measure'], ['cell'], ['in'], ['animals'], ['on'], ['in'], ['particles'], ['os', '##te', '##omy', '##eli', '##tis'], ['day', '-', 'time'], ['xi'], ['appropriate'], ['residents'], ['concentrations'], ['secondary'], ['real', '##ign', '##ing'], ['the'], ['of'], ['of'], ['our'], ['reporter'], ['statistical'], ['of'], ['cl', '##in'], [')'], ['also'], ['fitness'], ['bleeding'], ['therapy'], ['identification'], ['aroma', '##tase'], ['samples'], ['dona', '##ting'], ['per', '##me', '##ability'], ['%'], ['challenges'], ['factors'], ['in'], ['it'], ['24'], ['yang'], ['and'], ['.'], ['d', '##2'], ['boys'], ['our'], ['of'], ['in'], ['the'], [','], ['and'], ['might'], ['ramp'], ['are'], [')'], ['in'], ['hc', '##v'], ['services'], ['that'], ['rub', '##ella'], ['a'], ['the'], ['vascular'], ['swelling'], ['developed'], [')'], ['these'], ['both'], ['also'], ['i', '.', 'e', '.'], ['('], ['efficiency'], ['instability'], ['of'], ['advanced', '/', 'rec', '##urrent'], ['transplant'], ['black'], ['assessed'], ['the'], ['by'], ['.'], ['32'], ['should'], ['damage'], ['opinion'], ['studying'], ['quasi', '-', 'elastic'], ['65', '.', '4'], ['ps', '##s'], ['regions'], ['and'], ['we'], ['hoped'], ['homo', '##tet', '##ram', '##er'], ['vi', '##ment', '##in'], ['in'], ['comparisons'], ['review'], ['potent'], ['titanium'], ['simple'], ['.'], ['the'], ['hs', '##c'], ['.'], ['diagnosis'], ['ratio'], ['.'], ['.'], ['more'], ['est', '##imating'], ['presented'], ['their'], ['signal', '-', 'regulated'], ['ligand'], ['('], ['per', '##me', '##ability'], ['of'], ['the'], ['migration'], [','], ['rate'], ['%'], ['video', '##dis', '##c'], ['valley'], ['complement'], ['concern'], ['exposure'], ['administered'], ['and'], ['.'], ['bitter'], ['in'], ['outcomes'], ['preference'], ['labelled'], ['.'], ['that'], ['modification'], [','], [','], ['for'], ['cam'], ['the'], ['intro', '##n'], ['german'], ['to'], ['at'], ['a'], ['.'], ['challenge'], ['using'], ['rose'], ['micro', '##dis', '##se', '##cted'], [','], [','], ['.'], ['base'], ['cells'], ['predominantly'], ['this'], ['are'], ['de'], ['for'], ['4', '-', 'y', '##r'], ['chang'], ['a'], ['in'], ['displaced'], ['increase'], ['expression'], ['other'], ['short'], ['were'], ['hr'], ['that'], ['vent', '##ric', '##ular'], ['the'], ['%'], ['the'], ['from'], ['of'], ['tissues'], ['lip', '##osa', '##rco', '##ma'], ['com', '##t', 'with'], ['and'], ['the'], ['are'], ['cho', '##les', '##tat', '##ic'], ['slightly'], ['infection'], ['of'], ['125'], ['('], ['.'], ['did'], ['survival'], ['pro', '##kin', '##etic'], ['of'], ['sustained'], ['annual'], ['partly'], ['serum'], ['fluorescent'], ['flu', '##ma', '##zen', '##il'], [')'], ['not'], ['.'], ['e'], ['relation'], ['sociological'], ['not'], ['('], ['se', '##dent', '##ary'], ['the'], ['('], ['4'], ['('], ['from'], ['markers'], ['from'], ['following'], ['on'], ['development'], [','], ['data'], [','], ['8', '.', '5', '/', '100', ',', '000', '/', 'y', '##r'], ['a'], [','], ['when'], ['complete'], ['usually'], ['our'], ['mice'], ['the'], ['understanding'], ['however'], ['an'], ['especially'], ['auto', '##im', '##mun', '##e'], ['newborn', '##s'], ['mimic', '##king'], ['en', '##tre'], ['effects'], ['showing'], ['5'], ['in'], ['from'], ['['], ['pregnancy'], ['ii'], ['inhibit', '##ing'], ['by'], ['and'], ['to'], ['are'], ['produced', 'of'], ['a'], ['programme'], ['in'], ['injury'], ['were'], ['double'], ['blocks'], ['for'], ['cells'], ['non', '-', 'st', '-', 'elevation'], ['per', '##se', '##cu', '##cia', '##³', '##n'], ['which'], ['was'], ['.', '.', '.'], ['how'], ['either'], ['inform'], ['143'], ['three'], ['quality'], ['recommended'], ['lines'], ['of'], ['secondary'], ['major'], ['was'], ['or'], ['oxidation'], ['the'], ['with'], ['agent', 'use'], ['and'], ['response'], ['fault'], ['re', '##ani', '##mation'], ['we'], ['15'], ['met', '##hani', '##mini', '##um'], ['parameters'], ['and'], ['is'], [','], ['a'], ['than'], ['indicated'], [')'], ['actions'], ['architecture'], ['detected'], ['gut'], ['resonance'], [';'], ['the'], ['function'], ['assign', '##ing'], ['.'], ['was'], ['to'], ['with'], ['1', '-'], ['the'], ['dia', '##bet', '##ic'], [','], ['e', '/', 'at'], ['pro', '##te', '##omic'], ['.'], ['negatively'], ['and'], ['detect', '##able'], ['.'], ['of'], ['-'], ['of'], ['weeks'], ['rabbits'], ['developed'], ['brass', '##ica'], ['g', '##2', '/', 'm'], ['patients'], [','], ['palm'], ['fe', '##rti', '##lization'], ['suggested'], ['in'], ['helped'], ['potentially'], [','], ['-', '3'], [','], ['lens'], ['cells'], ['include'], ['in'], [','], ['anti', '##co', '##ag', '##ula', '##nts'], ['and'], ['provides'], ['poly'], ['found'], ['cross', '-', 'linked'], ['system'], ['experiment'], ['d', '##pp', '##1', '/', 'cat', '##he', '##ps', '##in'], ['prescription'], ['conducted'], ['ve', '##sic', '##ular'], ['of'], ['un'], [','], ['single', '-', 'centre'], ['pixels'], ['blend'], ['0', '.', '01'], ['generic'], ['does'], ['of'], ['was'], ['as'], ['of'], ['multi', '##mers'], ['sequence'], ['into'], ['.'], ['cad'], ['some'], ['pc', '##c'], ['ng', '/', 'ml'], ['scalp'], ['and'], ['before']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XZArd4RiA-F1",
        "colab_type": "text"
      },
      "source": [
        "Evaluate The Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zVxN9zNBBKp",
        "colab_type": "code",
        "outputId": "721d29bc-aa5b-4ff5-c76c-47d19041dd10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "model.eval()\n",
        "predictions = []\n",
        "true_labels = []\n",
        "eval_loss, eval_accuracy = 0, 0\n",
        "nb_eval_steps, nb_eval_examples = 0, 0\n",
        "for batch in valid_dataloader:\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n",
        "                              attention_mask=b_input_mask, labels=b_labels)\n",
        "        logits = model(b_input_ids, token_type_ids=None,\n",
        "                       attention_mask=b_input_mask)\n",
        "        \n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "    true_labels.append(label_ids)\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "\n",
        "    eval_loss += tmp_eval_loss.mean().item()\n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "    nb_eval_examples += b_input_ids.size(0)\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "pred_tags = [[tags_vals[p_i] for p_i in p] for p in predictions]\n",
        "valid_tags = [[tags_vals[l_ii] for l_ii in l_i] for l in true_labels for l_i in l ]\n",
        "print(\"Validation loss: {}\".format(eval_loss/nb_eval_steps))\n",
        "print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n",
        "print(\"Validation F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))\n",
        "\n",
        "#print(predictions)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation loss: 0.0017941044031787264\n",
            "Validation Accuracy: 0.9995652173913044\n",
            "Validation F1-Score: 0.5238095238095238\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJHGLu0HBCIL",
        "colab_type": "code",
        "outputId": "54bbdf09-1139-4c9d-fff2-6bc6d0fb1455",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwHEk6Gpot6y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}